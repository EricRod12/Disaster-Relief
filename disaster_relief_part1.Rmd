---
title: "disaster relief part 1"
author: "Eric Rodriguez"
date: "2024-03-16"
output: html_document
---


```{r hide-code, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

```{r warning=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(discrim)
library(patchwork)
#install.packages("glmnet")
library(glmnet)
#install.packages("doParallel")
library(dplyr)
library(doParallel)
library(caret)
library(rsample)
library(MASS) 
library(discrim)
library(yardstick)
library(GGally)
```

```{r}
train <- read.csv(file="C:/Users/ericr/Downloads/HaitiPixels.csv")
```

[4:52 PM] Faparusi, Ade A (vvk6rd)


```{r}
 
#Boxplots of response variable vs predictor variable

train %>%

pivot_longer(!Blue_Tarp, names_to='variable', values_to='value') %>%

ggplot(aes(x=Blue_Tarp, y=value)) +

geom_boxplot() +

facet_wrap(~variable, scales='free')

```
```{r}
train$Class <- as.factor(train$Class)
```

```{r}
train$Blue_Tarp <- ifelse(train$Class == "Blue Tarp", 1, 0)
```

```{r}
train$Blue_Tarp <- as.factor(train$Blue_Tarp)
```

```{r}
train <- subset(train, select = -Class)

```

```{r}

#Predictor variables correlation

train%>%dplyr::select(-c(Blue_Tarp))%>%

ggpairs(aes(alpha=0.1),progress = FALSE)

```

```{r}
formula <- Blue_Tarp ~`Red`+`Green`+`Blue`
```


```{r}
# View the value counts of the Blue_Tarp column in train
table(train$Blue_Tarp)

```



```{r}
resamples <- vfold_cv(train, v=10, strata=Blue_Tarp)
metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)
```

```{r}
logreg_spec <- logistic_reg(mode="classification") %>%
set_engine('glm')
lda_spec <- discrim_linear(mode="classification") %>%
set_engine('MASS')
qda_spec <- discrim_quad(mode="classification") %>%
set_engine('MASS')
```

```{r}
dis_recipe <- recipe(formula, data=train) %>%
step_normalize(all_numeric_predictors())
```

```{r}
logreg_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(logreg_spec)
lda_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(lda_spec)
qda_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(qda_spec)


```

```{r}
logreg_cv <- fit_resamples(logreg_wf, resamples, metrics=metrics, control=cv_control)
lda_cv <- fit_resamples(lda_wf, resamples, metrics=metrics, control=cv_control)
qda_cv <- fit_resamples(qda_wf, resamples, metrics=metrics, control=cv_control)
```


```{r}
cv_metrics <- bind_rows(
collect_metrics(logreg_cv) %>% mutate(model="Logistic regression"),
collect_metrics(lda_cv) %>% mutate(model="LDA"),
collect_metrics(qda_cv) %>% mutate(model="QDA")
)

```

```{r}
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean-std_err, xmax=mean+std_err)) +
geom_point() +
geom_linerange() +
facet_wrap(~ .metric)

```


Above are the metrics for each model (Logistic Regression, LDA, and QDA) before threshold optimization. We will now perform 10 fold cross validation while optimizing the J index for each fold. The reason for choosing the J-index as a metric for threshold optimization is that it is often more reliable than something like accuracy when there are extremely unbalanced classes for the response variable. We created a function called find_optimal_threshold that iterates over each of the folds and calculates the J-index of each fold and finds the threshold that maximizes the J-index.
```{r}
#writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
#install.packages("probably")
#pak::pak("probably")

library(probably)
#pkgbuild::check_build_tools(debug = TRUE)
#install.packages("pak")
library(pkgbuild)
#install.packages("pkgbuild")
#pak::pak("tidymodels/probably")
#pkgbuild::check_build_tools(debug = TRUE)
threshold_graph <- function(model_cv, model_name) {
performance <- probably::threshold_perf(collect_predictions(model_cv), Blue_Tarp, .pred_1,
thresholds=seq(0.00, 1.00, 0.01), event_level="second",
metrics=metric_set(j_index, accuracy, kap))
max_metrics <- performance %>%
group_by(.metric) %>%
filter(.estimate == max(.estimate))
ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
geom_line() +
geom_point(data=max_metrics, color="black") +
labs(x="Threshold", y="Metric value") +
coord_cartesian(ylim=c(0, 1.0))
}
g1 <- threshold_graph(logreg_cv, "Logistic regression")
g2 <- threshold_graph(lda_cv, "LDA")
g3 <- threshold_graph(qda_cv, "QDA")


```

```{r, fig.width=4, fig.height=7}
combined_plot <- g1/ g2 /g3
combined_plot
```

Above are the threshold optimization metrics. The three graphs are showing the results of our function that calculates performance metrics, such as accuracy, j_index, and kap for different metrics within the specified range of 0.05 and 0.95. The dots represent the highest threshold value for each metric.

```{r}
calculate_conf_mat <- function(model_cv,threshold) {
collect_predictions(model_cv) %>%
mutate(
.pred_1 = if_else(.pred_1>threshold, 'high', 'low'),
.pred_class = factor(.pred_class, levels=c(0, 1))
) %>%
conf_mat(truth=Blue_Tarp, estimate=.pred_class)
}

#me<-collect_predictions(logreg_cv)
```


```{r}
find_optimal_threshold <- function(predictions, truth) {
  # Create a range of thresholds
  thresholds <- seq(0, 1, by = 0.01)
  
  # Initialize vectors to store J-index values for each threshold
  j_index_values <- numeric(length(thresholds))
  
  # Iterate over each threshold
  for (i in seq_along(thresholds)) {
    # Convert probabilities to binary predictions based on the threshold
    binary_predictions <- ifelse(predictions > thresholds[i], TRUE, FALSE)
    
    # Calculate confusion matrix
    confusion_matrix <- table(binary_predictions, truth)
    
    # Ensure there are enough samples in each class for calculations
    if (ncol(confusion_matrix) != 2 || nrow(confusion_matrix) != 2) {
      j_index_values[i] <- 0  # Set J-index to 0 if confusion matrix dimensions are not as expected
    } else if (sum(confusion_matrix[2, ]) == 0 || sum(confusion_matrix[, 2]) == 0) {
      j_index_values[i] <- 0  # Set J-index to 0 if sensitivity or specificity is undefined
    } else {
      # Calculate sensitivity and specificity
      sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
      specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
      
      # Calculate J-index
      j_index_values[i] <- sensitivity + specificity - 1
    }
  }
  
  # Find the optimal threshold that maximizes the J-index
  optimal_threshold <- thresholds[which.max(j_index_values)]
  optimal_j_index <- max(j_index_values)
  
  return(list(optimal_threshold = optimal_threshold, optimal_j_index = optimal_j_index))
}
```

```{r}
calculate_metrics_at_threshold <- function(predictions, truth, threshold, data = "train") {
  binary_predictions <- ifelse(predictions > threshold, TRUE, FALSE)
  confusion_matrix <- table(binary_predictions, truth)
  
  # Ensure there are enough samples in each class for calculations
  if (ncol(confusion_matrix) != 2 || nrow(confusion_matrix) != 2) {
    return(list(accuracy = 0, sensitivity = 0, specificity = 0, j_index = 0))  # Set all metrics to 0 if confusion matrix dimensions are not as expected
  } else if (sum(confusion_matrix[2, ]) == 0 || sum(confusion_matrix[, 2]) == 0) {
    return(list(accuracy = 0, sensitivity = 0, specificity = 0, j_index = 0))  # Set all metrics to 0 if sensitivity or specificity is undefined
  } else {
    # Calculate accuracy
    accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
    
    # Calculate sensitivity
    sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
    
    # Calculate specificity
    specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
    
    # Calculate J-index
    j_index <- sensitivity + specificity - 1
    
    precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
    
    # Print whether it's for train or test data
    cat("Metrics for", data, "data at threshold", threshold, ":\n")
    cat("Accuracy:", accuracy, "\n")
    cat("Sensitivity:", sensitivity, "\n")
    cat("Specificity:", specificity, "\n")
    cat("J-index:", j_index, "\n")
    cat("Precision:", precision, "\n")
        
    return(list(accuracy = accuracy, sensitivity = sensitivity, specificity = specificity, j_index = j_index, precision = precision))
  }
}
```

We perform 10 fold cross validation and threshold optimization with J-index as the metric for LDA and QDA. We are creating 10 fold cross validation splits from the training set. The data is divided into 10 parts, with each part used once as a validation set while the rest serve as a training set. We are ensuring the proportion of the two classes that is approximately equal within each fold, which is particularly useful when handling imbalanced datasets. Below are the results for each fold and for each model. 

```{r, results='hide', message=FALSE, warning=FALSE}
library(purrr)


# Define the resampling procedure
set.seed(123)
folds <- vfold_cv(train, v = 10, strata = Blue_Tarp)



```
First we define the resampling procedure. Then, we perform cross validation. We fit the model on the fold's training data, predict on the fold's test data. Then, we find the optimal threshold for each fold, calculate metrics at the optimal threshold, and then store the metrics and the optimal threshold for each fold.

We created a function called perform_cv_with_threshold for our three models, which allows us to assess our models performance at our optimal threshold during 10 fold CV by leveraging out find_optimal_threshold and calculate_metrics_at_threshold functions. In other words, the function goes through each fold, finds the best threshold for that particular fold based on the best J index, and then we are able to have the results of each fold for each model. 
```{r, results='hide', message=FALSE, warning=FALSE}
perform_cv_with_threshold <- function(model_spec, folds, dis_recipe) {
  fold_results <- list()
  metrics_list <- list(accuracy = numeric(), sensitivity = numeric(), specificity = numeric(), j_index = numeric(), precision = numeric())
  optimal_thresholds <- numeric()
  
  for (i in 1:length(folds$splits)) {
    fold_data <- assessment(folds$splits[[i]])
    fold_train_data <- analysis(folds$splits[[i]])
    
    fitted_model <- workflow() %>%
      add_recipe(dis_recipe) %>%
      add_model(model_spec) %>%
      fit(data = fold_train_data)
    
    predictions <- predict(fitted_model, new_data = fold_data, type = "prob") %>%
      bind_cols(fold_data)
    
    optimal_threshold_info <- find_optimal_threshold(predictions$.pred_1, fold_data$Blue_Tarp)
    optimal_thresholds <- c(optimal_thresholds, optimal_threshold_info$optimal_threshold)
    
    metrics <- calculate_metrics_at_threshold(predictions$.pred_1, fold_data$Blue_Tarp, optimal_threshold_info$optimal_threshold, "validation")
    
    # Storing each metric in metrics_list for later averaging
    metrics_list$accuracy <- c(metrics_list$accuracy, metrics$accuracy)
    metrics_list$sensitivity <- c(metrics_list$sensitivity, metrics$sensitivity)
    metrics_list$specificity <- c(metrics_list$specificity, metrics$specificity)
    metrics_list$j_index <- c(metrics_list$j_index, metrics$j_index)
    metrics_list$precision<-c(metrics_list$precision, metrics$precision)
  }
  
  # Calculate the mean of the metrics and optimal thresholds
  #mean_metrics <- lapply(metrics_list, mean)
  mean_metrics <- lapply(metrics_list, function(x) mean(x, na.rm = TRUE))
  mean_optimal_threshold <- mean(optimal_thresholds)
  
  return(list(mean_metrics = mean_metrics, mean_optimal_threshold = mean_optimal_threshold))
}

results_logreg <- perform_cv_with_threshold(logreg_spec, folds, dis_recipe)
print("Logistic Regression - Mean Metrics:")
print(results_logreg$mean_metrics)
print(sprintf("Mean Optimal Threshold: %f", results_logreg$mean_optimal_threshold))



```
```{r}
results_lda <- perform_cv_with_threshold(lda_spec, folds, dis_recipe)
print("LDA - Mean Metrics:")
print(results_lda$mean_metrics)
print(sprintf("Mean Optimal Threshold: %f", results_lda$mean_optimal_threshold))


```
```{r}
results_qda <- perform_cv_with_threshold(qda_spec, folds, dis_recipe)
print("QDA - Mean Metrics:")
print(results_qda$mean_metrics)
print(sprintf("Mean Optimal Threshold: %f", results_qda$mean_optimal_threshold))

```

Below are the confusion matrices for each model at the mean optimal threshold. 

```{r}
print(results_logreg$mean_optimal_threshold)
cmlogreg<-calculate_conf_mat(logreg_cv,results_logreg$mean_optimal_threshold)
print(cmlogreg)

```
TPR_LR:
```{r}
TPR <- cmlogreg$table[2,2] / (cmlogreg$table[2,2] + cmlogreg$table[1,2]) # TP / (TP + FN)
print(TPR)

```
FPR_LR:
```{r}
FPR <- cmlogreg$table[2,1] / (cmlogreg$table[2,1] + cmlogreg$table[1,1])
print(FPR)
```

```{r}
cmlda<-calculate_conf_mat(lda_cv,results_lda$mean_optimal_threshold)
print(cmlda)
```
TPR_LDA:
```{r}
TPR1 <- cmlda$table[2,2] / (cmlda$table[2,2] + cmlda$table[1,2]) # TP / (TP + FN)
print(TPR1)
```

FPR_LDA:

```{r}
FPR1 <- cmlda$table[2,1] / (cmlda$table[2,1] + cmlda$table[1,1])
print(FPR1)
```

```{r}
cmqda<-calculate_conf_mat(qda_cv,results_qda$mean_optimal_threshold)
print(cmqda)
```
TPR_QDA:

```{r}
TPR2 <- cmqda$table[2,2] / (cmqda$table[2,2] + cmqda$table[1,2]) # TP / (TP + FN)
print(TPR2)
```

FPR_QDA:
```{r}
FPR2 <- cmlda$table[2,1] / (cmlda$table[2,1] + cmlda$table[1,1])
print(FPR2)
```
From our confusion matrices, we see that the performances are solid, verifying what we saw from our mean performance metrics across the 10 folds for each model. 

```{r}
roc_cv_plot <- function(model_cv, model_name) {
cv_predictions <- collect_predictions(model_cv)
cv_ROC <- cv_predictions %>%
roc_curve(truth=Blue_Tarp, .pred_1, event_level="second")
autoplot(cv_ROC) +
labs(title=model_name)
}
g1 <- roc_cv_plot(logreg_cv, "Logistic regression")
g2 <- roc_cv_plot(lda_cv, "LDA")
g3 <- roc_cv_plot(qda_cv, "QDA")
g1 + g2 + g3
```

```{r}
roc_cv_data <- function(model_cv) {
cv_predictions <- collect_predictions(model_cv)
cv_predictions %>%
roc_curve(truth=Blue_Tarp, .pred_1, event_level="second")
}
bind_rows(
roc_cv_data(logreg_cv) %>% mutate(model="Logistic regression"),
roc_cv_data(lda_cv) %>% mutate(model="LDA"),
roc_cv_data(qda_cv) %>% mutate(model="QDA"),

) %>%
ggplot(aes(x=1-specificity, y=sensitivity, color=model)) +
geom_line()
```
There is no real need to try and optimize the above models (Logstic, QDA, and LDA) further, with step_pca as predictors, or in other words, try to reduce the dimensionality of our dataset further. The reason is because with a dataset with only 3 predictors, there will be diminishing returns from trying to use step_pca as predictors. We already have excellent performance metrics without step_pca.

KNN model

Next we will do the KNN model. We will explore a range of K values between 1 and 30 and try to assess the best value for k based on metrics and performance with each k value within our 10 folds. We are also going to normalize the data prior to fitting the model.

```{r}
# Define the recipe
rec <- recipe(Blue_Tarp ~ ., data = train) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric())

# Define the model specification
knn_spec <- nearest_neighbor(weight_func = "rectangular", neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Define the resampling method
cv <- vfold_cv(train, v = 10, strata=Blue_Tarp)

# Define the workflow
knn_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(knn_spec)

# Define the tuning grid
gridvals <- tibble(neighbors = seq(1, 30, by = 1))

metrics1=metric_set(roc_auc)
control=control_grid(save_pred = TRUE)
# Tune the model
knn_tuned <- tune_grid(
  knn_wf,
  resamples = cv,
  grid = gridvals,
  metrics = metrics1,
  control = control
)

# Extract the best model
best_knn <- select_best(knn_tuned, "roc_auc")

tuned_nn_model <-finalize_workflow(knn_wf,best_knn) %>%
fit(train)

augmented_data <- tuned_nn_model %>%
  augment(new_data = train)


```

This model is outperforming the previous three models, especially in accuracy and specificity. The performance among k values is relatively the same. It doesn't truly matter which k value we choose, because the knn model is performing well for all k values chosen, but if we had to, I would choose the k that has the highest J index. The specificity and accuracy are extremely high for this "best" value of k.

based on J-index. J index is a better metric for imbalanced classes when comparing values for k in knn.
This model has excellent performance metrics seemingly (accuracy, sensitivity, and specificity). We are training the model with the best_k.

 Below are the selection results of the 10 fold CV based on maximizing J index.


```{r}
optimal_threshold_KNN_train <- find_optimal_threshold(augmented_data$.pred_1, augmented_data$Blue_Tarp)

print(optimal_threshold_KNN_train)

```
Below are the metrics for the train data set at the optimal threshold of .9 for our KNN model.

```{r}
metrics_KNN_train <- calculate_metrics_at_threshold(augmented_data$.pred_1, augmented_data$Blue_Tarp, optimal_threshold_KNN_train$optimal_threshold)

print(metrics_KNN_train)
```
The above performance metrics seem good, however, we need to evaluate the confusion matrix at this optimal threshold. 

```{r}
predicted_class <- ifelse(augmented_data$.pred_1 > optimal_threshold_KNN_train$optimal_threshold, "1", "0")

predictions <- tibble(
  .pred_class = as.factor(predicted_class),
  .truth = as.factor(augmented_data$Blue_Tarp)
)

# Create the confusion matrix
confusion_matrix <- conf_mat(predictions, truth = .truth, estimate = .pred_class)

# Print the confusion matrix
print(confusion_matrix)

```

```{r}
TPR6 <- confusion_matrix$table[2,2] / (confusion_matrix$table[2,2] + confusion_matrix$table[1,2]) # TP / (TP + FN)
print(TPR6)
```

FPR_QDA:
```{r}
FPR6 <- confusion_matrix$table[2,1] / (confusion_matrix$table[2,1] + confusion_matrix$table[1,1])
print(FPR6)
```
We see that with threshold optimization, our confusion matrix produces excellent results. This means it identified many of the positive cases correctly. It also was able to identify most negative cases as negative. since we have been able to distinguish between the two classes well with knn based on the confusion matrix, we will say it is effective for our purposes.
```{r}
roc_data_knn <- roc_curve(augmented_data, truth = Blue_Tarp, .pred_1, event_level = "second")

roc_data_knn$model <- "KNN"

roc_plot_knn <- ggplot(roc_data_knn, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(x = "1 - Specificity (False Positive Rate)", y = "Sensitivity (True Positive Rate)",
       title = "ROC Curve Comparison for Test Sets") +
  theme_minimal() +
  scale_color_manual(values = c("black", "green", "red", "orange", "yellow", "purple")) # Customize colors as needed

print(roc_plot_knn)
```

Here, we have a great looking ROC curve. In addition, we have a low AUC and high accuracy for the optimal. The confusion matrix also shows a great TPR and low FPR. Our good results from the confusion matrix verify the high performance metrics with 10 fold CV and threshold optimization as well as our ROC curve.

For our elasticnet penalized logistic regression, we first have to prepare the data for glmnet. This means turning our normalized train data into a matrix.
```{r}
# Define the model specification
penalized_logit <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")


# Define the workflow
penalized_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(penalized_logit)

# Define the tuning grid
penalized_grid <- expand.grid(
  penalty = c(0.1, 0.2, 0.3), # Range of alpha values for elastic net
  mixture = seq(0.1, 0.9, by = 0.1) # Range of lambda values for elastic net
)

# Tune the model
penalized_tuned <- tune_grid(
  penalized_wf,
  resamples = cv,
  grid = penalized_grid,
  metrics = metric_set(roc_auc),
  control = control_grid(verbose = TRUE)
)

# Extract the best model
best_penalized <- select_best(penalized_tuned, "roc_auc")

optimal_parameters <- best_penalized$.config
# Train the final model with optimal parameters
final_penalized_model <- finalize_workflow(penalized_wf, best_penalized ) %>%
  fit(train)

# Predict probabilities on the train set
train_predictions <- augment(final_penalized_model, new_data = train)

# Find the optimal threshold
optimal_threshold_pen <- find_optimal_threshold(train_predictions$.pred_1, train$Blue_Tarp)

# Calculate metrics at the optimal threshold
metrics_at_threshold_pen <- calculate_metrics_at_threshold(train_predictions$.pred_1, train_predictions$Blue_Tarp, optimal_threshold_pen$optimal_threshold)

cat("Optimal Threshold:", optimal_threshold_pen$optimal_threshold, "\n")
print(metrics_at_threshold_pen)

# Plot ROC curve
roc_data_pen <- roc_curve(train_predictions, truth = Blue_Tarp, .pred_1, event_level = "second")

roc_data_pen$model <- "PEN"

roc_plot_pen <- ggplot(roc_data_pen, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(x = "1 - Specificity (False Positive Rate)", y = "Sensitivity (True Positive Rate)",
       title = "ROC Curve Comparison for Test Sets") +
  theme_minimal() +
  scale_color_manual(values = c("black", "green", "red", "orange", "yellow", "purple")) # Customize colors as needed

print(roc_plot_pen)

```


Above are the model selection results, or in other words, the values of alpha and lambda that produced the highest roc value on the train set. Now we evaluate the model on the test set by using the trained model.

```{r}


roc_cv_data2 <- function(model_cv) {
cv_predictions <-model_cv$pred
cv_predictions %>%
roc_curve(truth=obs, True, event_level="second")
}

```

```{r}
predicted_class1 <- ifelse(train_predictions$.pred_1 > optimal_threshold_pen$optimal_threshold, "1", "0")

predictions1 <- tibble(
  .pred_class = as.factor(predicted_class1),
  .truth = as.factor(train_predictions$Blue_Tarp)
)

# Create the confusion matrix
confusion_matrix_enet <- conf_mat(predictions1, truth = .truth, estimate = .pred_class)

# Print the confusion matrix
print(confusion_matrix_enet)
```

TPR_enet: 

```{r}
TPR5 <- confusion_matrix_enet$table[2,2] / (confusion_matrix_enet$table[2,2] + confusion_matrix_enet$table[1,2]) # TP / (TP + FN)
print(TPR5)
```

FPR_enetL:
```{r}
FPR5 <- confusion_matrix_enet$table[2,1] / (confusion_matrix_enet$table[2,1] + confusion_matrix_enet$table[1,1])
print(FPR5)
```

We have a small FPR but our TPR is not as high as logistic regression. We will therefore say that our KNN is the best ultimately because of the high TPR and low FPR. A close contender was the logistic regression, which performed above 95% in key metrics such as accuracy, sensitivity, and specificity when looking at the mean performance across all 10 folds. It performed better than LDA and QDA when looking at these metrics alone. It also performed better in terms of TPR and FPR than LDA and QDA. The TPR is arguably the most important metric for our purposes, because we need to correctly identify the positive cases for our disaster relief efforts. Furthermore, the Logistic Regression is less computationally exhaustive and faster to run than the knn and penalized logistic regression. This may be a huge factor in future disaster relief efforts to consider.
```{r}
# Get ROC curve data for logistic regression, LDA, and QDA
roc_logreg <- roc_cv_data(logreg_cv) %>% mutate(model = "Logistic regression")
roc_lda <- roc_cv_data(lda_cv) %>% mutate(model = "LDA")
roc_qda <- roc_cv_data(qda_cv) %>% mutate(model = "QDA")

# Combine all ROC curve data
roc_data <- bind_rows(
  roc_data_pen %>% mutate(model = "Penalized Logistic Regression"),
  roc_data_knn %>% mutate(model = "KNN"),
  roc_logreg,
  roc_lda,
  roc_qda
)

# Plot ROC curves
ggplot(roc_data, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() +
  labs(x = "False Positive Rate", y = "True Positive Rate", color = "Model") +
  theme_minimal()
```

```{r}
library(ranger)

random_wf <- workflow() %>%
add_recipe(recipe(formula, data=train)) %>%
add_model(rand_forest(mode="classification", mtry=tune(), min_n=tune()) %>%
set_engine("ranger", importance="impurity"))
parameters <- extract_parameter_set_dials(random_wf)
parameters

parameters <- extract_parameter_set_dials(random_wf) %>%
update(mtry = mtry(c(2, 8)))

tune_random <- tune_bayes(random_wf,
resamples=resamples,
metrics=metrics,
param_info=parameters, iter=10)

autoplot(tune_random)
```

```{r}
# Extract the best model
best_random<-select_best(tune_random, metric="roc_auc")

best_random_model <- finalize_workflow(random_wf, best_random)%>%
  fit(train)

optimal_parameters1 <- best_random_model$.config

# Predict probabilities on the train set
train_predictions1 <- augment(best_random_model, new_data = train)

# Find the optimal threshold
optimal_threshold_random <- find_optimal_threshold(train_predictions1$.pred_1, train$Blue_Tarp)

# Calculate metrics at the optimal threshold
metrics_at_threshold_random <- calculate_metrics_at_threshold(train_predictions1$.pred_1, train_predictions1$Blue_Tarp, optimal_threshold_random$optimal_threshold)

cat("Optimal Threshold:", optimal_threshold_random$optimal_threshold, "\n")
print(metrics_at_threshold_random)

# Plot ROC curve
roc_data_random <- roc_curve(train_predictions1, truth = Blue_Tarp, .pred_1, event_level = "second")

roc_data_random$model <- "RANDOM"

roc_plot_random <- ggplot(roc_data_random, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() +
  geom_abline(linetype = "dashed", color = "gray") +
  labs(x = "1 - Specificity (False Positive Rate)", y = "Sensitivity (True Positive Rate)",
       title = "ROC Curve Comparison for Test Sets") +
  theme_minimal() +
  scale_color_manual(values = c("black", "green", "red", "orange", "yellow", "purple")) # Customize colors as needed

print(roc_plot_random)

```
```{r}
predicted_class2 <- ifelse(train_predictions1$.pred_1 > optimal_threshold_random$optimal_threshold, "1", "0")

predictions2 <- tibble(
  .pred_class = as.factor(predicted_class2),
  .truth = as.factor(train_predictions1$Blue_Tarp)
)

# Create the confusion matrix
confusion_matrix_random <- conf_mat(predictions2, truth = .truth, estimate = .pred_class)

# Print the confusion matrix
print(confusion_matrix_random)
```
RANDOM_TPR:
```{r}
TPR7 <- confusion_matrix_random$table[2,2] / (confusion_matrix_random$table[2,2] + confusion_matrix_random$table[1,2]) # TP / (TP + FN)
print(TPR7)
```

FPR_RANDOM:
```{r}
FPR7 <- confusion_matrix_random$table[2,1] / (confusion_matrix_random$table[2,1] + confusion_matrix_random$table[1,1])
print(FPR7)
```

```{r}
rec1<-recipe(formula, data=train) %>%
step_normalize(all_numeric_predictors())

svm_linear_spec <- svm_linear(mode = "classification", cost = tune(), margin=tune()) %>%
  set_engine("kernlab")

svm_poly_spec <- svm_poly(mode = "classification", cost = tune(), margin = tune(), degree = tune()) %>%
  set_engine("kernlab")

svm_rbf_spec <- svm_rbf(cost = tune(), margin = tune(), rbf_sigma = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

workflow_linear1 <- workflow() %>%
  add_model(svm_linear_spec)  %>%
  add_recipe(rec1)

workflow_poly1 <- workflow() %>%
  add_model(svm_poly_spec) %>%
  add_recipe(rec1)

workflow_rbf1 <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(rec1)

```


```{r}
control <- control_bayes(verbose = TRUE, save_pred = TRUE)
bayes_results_linear1 <- tune_bayes(
  workflow_linear1,
  resamples = resamples,
  metrics = metrics1,
  control = control
)

svm_poly_params <- parameters(svm_poly_spec) %>%
  update(
    degree = degree(c(2, 5))
  )


bayes_results_poly1 <- tune_bayes(
  workflow_poly1,
  resamples = resamples,
  metrics = metrics1,
  control = control,
  parameters=svm_poly_params
)

parameters <- extract_parameter_set_dials(workflow_rbf1) %>%
update(rbf_sigma = rbf_sigma(range = c(-4, 0), trans = log10_trans()))

set.seed(234)  # For reproducibility
tuning_results1 <- tune_bayes(
  workflow_rbf1,
  resamples = resamples,
  parameters = parameters,
  control = control,
  metrics=metrics1
)

show_notes(bayes_results_linear1)

show_notes(.Last.tune.result)
best_linear1 <- select_best(bayes_results_linear1, "roc_auc")
best_poly1 <- select_best(bayes_results_poly1, "roc_auc")
best_rbf1 <- select_best(tuning_results1, "rmse")

print(best_linear1)
print(best_poly1)
print(best_rbf1)
```



