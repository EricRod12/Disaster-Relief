---
title: "part1 Disaster Relief"
author: "Eric Rodriguez"
date: "2024-03-13"
output: html_document
---

```{r hide-code, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

```{r}
library(tidymodels)
library(tidyverse)
library(discrim)
library(patchwork)
#install.packages("glmnet")
library(glmnet)
#install.packages("doParallel")
library(dplyr)
library(doParallel)
library(caret)
library(rsample)
library(MASS) 
library(discrim)
library(yardstick)
```

```{r}
df <- read.csv(file="C:/Users/ericr/Downloads/HaitiPixels.csv")
```

```{r}
df$Class <- as.factor(df$Class)
```

```{r}
df$Blue_Tarp <- ifelse(df$Class == "Blue Tarp", 1, 0)
```

```{r}
df$Blue_Tarp <- as.factor(df$Blue_Tarp)
```

```{r}
df <- subset(df, select = -Class)

```

```{r}
set.seed(123)  # Set seed for reproducibility
disrelief_split <- initial_split(df, prop=0.8, strata=Blue_Tarp)
train <- training(disrelief_split)
test <- testing(disrelief_split)
```

```{r}
formula <- Blue_Tarp ~`Red`+`Green`+`Blue`
```

```{r}
logreg_model <- logistic_reg(mode="classification") %>% 
        set_engine("glm") %>% 
        fit(formula, train)

lda_model <- discrim_linear(mode="classification") %>% 
        set_engine("MASS") %>%
        fit(formula, train)
qda_model <- discrim_quad(mode="classification") %>% 
        set_engine("MASS") %>%
        fit(formula, train)
```


```{r}
# Check the output structure of the logistic regression model
#str(logreg_model)

```

```{r}
train$Blue_Tarp <- as.factor(train$Blue_Tarp)
test$Blue_Tarp <- as.factor(test$Blue_Tarp)
```

```{r}
# View the value counts of the Blue_Tarp column in train
table(train$Blue_Tarp)

```



```{r}
resamples <- vfold_cv(train, v=10, strata=Blue_Tarp)
metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)
```

```{r}
logreg_spec <- logistic_reg(mode="classification") %>%
set_engine('glm')
lda_spec <- discrim_linear(mode="classification") %>%
set_engine('MASS')
qda_spec <- discrim_quad(mode="classification") %>%
set_engine('MASS')
```

```{r}
dis_recipe <- recipe(formula, data=train) %>%
step_normalize(all_numeric_predictors())
```

```{r}
logreg_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(logreg_spec)
lda_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(lda_spec)
qda_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(qda_spec)

```

```{r}
logreg_cv <- fit_resamples(logreg_wf, resamples, metrics=metrics, control=cv_control)
lda_cv <- fit_resamples(lda_wf, resamples, metrics=metrics, control=cv_control)
qda_cv <- fit_resamples(qda_wf, resamples, metrics=metrics, control=cv_control)
```

```{r}
cv_metrics <- bind_rows(
collect_metrics(logreg_cv) %>% mutate(model="Logistic regression"),
collect_metrics(lda_cv) %>% mutate(model="LDA"),
collect_metrics(qda_cv) %>% mutate(model="QDA")
)

```

```{r}
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean-std_err, xmax=mean+std_err)) +
geom_point() +
geom_linerange() +
facet_wrap(~ .metric)

```


Above are the metrics for each model (Logistic Regression, LDA, and QDA) before threshold optimization. We will now perform 10 fold cross validation while optimizing the J index for each fold. The reason for choosing the J-index as a metric for threshold optimization is that it is often more reliable than something like accuracy when there are extremely unbalanced classes for the response variable. We created a function called find_optimal_threshold that iterates over each of the folds and calculates the J-index of each fold and finds the threshold that maximizes the J-index.

```{r}
find_optimal_threshold <- function(predictions, truth) {
  # Create a range of thresholds
  thresholds <- seq(0, 1, by = 0.01)
  
  # Initialize vectors to store J-index values for each threshold
  j_index_values <- numeric(length(thresholds))
  
  # Iterate over each threshold
  for (i in seq_along(thresholds)) {
    # Convert probabilities to binary predictions based on the threshold
    binary_predictions <- ifelse(predictions > thresholds[i], TRUE, FALSE)
    
    # Calculate confusion matrix
    confusion_matrix <- table(binary_predictions, truth)
    
    # Ensure there are enough samples in each class for calculations
    if (ncol(confusion_matrix) != 2 || nrow(confusion_matrix) != 2) {
      j_index_values[i] <- 0  # Set J-index to 0 if confusion matrix dimensions are not as expected
    } else if (sum(confusion_matrix[2, ]) == 0 || sum(confusion_matrix[, 2]) == 0) {
      j_index_values[i] <- 0  # Set J-index to 0 if sensitivity or specificity is undefined
    } else {
      # Calculate sensitivity and specificity
      sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
      specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
      
      # Calculate J-index
      j_index_values[i] <- sensitivity + specificity - 1
    }
  }
  
  # Find the optimal threshold that maximizes the J-index
  optimal_threshold <- thresholds[which.max(j_index_values)]
  optimal_j_index <- max(j_index_values)
  
  return(list(optimal_threshold = optimal_threshold, optimal_j_index = optimal_j_index))
}
```

```{r}
calculate_metrics_at_threshold <- function(predictions, truth, threshold, data = "train") {
  binary_predictions <- ifelse(predictions > threshold, TRUE, FALSE)
  confusion_matrix <- table(binary_predictions, truth)
  
  # Ensure there are enough samples in each class for calculations
  if (ncol(confusion_matrix) != 2 || nrow(confusion_matrix) != 2) {
    return(list(accuracy = 0, sensitivity = 0, specificity = 0, j_index = 0))  # Set all metrics to 0 if confusion matrix dimensions are not as expected
  } else if (sum(confusion_matrix[2, ]) == 0 || sum(confusion_matrix[, 2]) == 0) {
    return(list(accuracy = 0, sensitivity = 0, specificity = 0, j_index = 0))  # Set all metrics to 0 if sensitivity or specificity is undefined
  } else {
    # Calculate accuracy
    accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
    
    # Calculate sensitivity
    sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
    
    # Calculate specificity
    specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
    
    # Calculate J-index
    j_index <- sensitivity + specificity - 1
    
    # Print whether it's for train or test data
    cat("Metrics for", data, "data at threshold", threshold, ":\n")
    cat("Accuracy:", accuracy, "\n")
    cat("Sensitivity:", sensitivity, "\n")
    cat("Specificity:", specificity, "\n")
    cat("J-index:", j_index, "\n\n")
    
    return(list(accuracy = accuracy, sensitivity = sensitivity, specificity = specificity, j_index = j_index))
  }
}
```

We perform 10 fold cross validation and threshold optimization with J-index as the metric for LDA and QDA. We are creating 10 fold cross validation splits from the training set. The data is divided into 10 parts, with each part used once as a validation set while the rest serve as a training set. We are ensuring teh proportion of the two classes that is approximately equal within each fold, which is particularly useful when handling imbalanced datasets. Below are the results for each fold and for each model. 
```{r, results='hide', message=FALSE, warning=FALSE}
library(purrr)


# Define the resampling procedure
set.seed(123)
folds <- vfold_cv(train, v = 10, strata = Blue_Tarp)

# Initialize a list to store results
fold_results <- list()

# Perform the cross-validation
for (i in 1:length(folds$splits)) {
  # Get the train and test split for the current fold
  fold_data <- assessment(folds$splits[[i]])
  fold_train_data <- analysis(folds$splits[[i]])
  
  # Fit the model on the fold's training data
  fitted_model <- workflow() %>%
    add_recipe(dis_recipe) %>%
    add_model(logreg_spec) %>%
    fit(data = fold_train_data)
  
  # Predict on the fold's test data
  predictions <- predict(fitted_model, new_data = fold_data, type = "prob") %>%
    bind_cols(fold_data)
  
  # Find the optimal threshold for this fold
  optimal_threshold_info <- find_optimal_threshold(predictions$.pred_1, fold_data$Blue_Tarp)
  
  # Calculate metrics at the optimal threshold
  metrics <- calculate_metrics_at_threshold(predictions$.pred_1, fold_data$Blue_Tarp, optimal_threshold_info$optimal_threshold, "test")
  
  # Store the metrics and the optimal threshold for this fold
  fold_results[[i]] <- list(metrics = metrics, optimal_threshold = optimal_threshold_info$optimal_threshold)
}



```

```{r}
# Calculate mean of each metric across all folds
mean_accuracy <- mean(sapply(fold_results, function(x) x$metrics$accuracy))
mean_sensitivity <- mean(sapply(fold_results, function(x) x$metrics$sensitivity))
mean_specificity <- mean(sapply(fold_results, function(x) x$metrics$specificity))
mean_j_index <- mean(sapply(fold_results, function(x) x$metrics$j_index))

# Print the aggregated metrics
#cat("Mean Accuracy:", mean_accuracy, "\n")
#cat("Mean Sensitivity:", mean_sensitivity, "\n")
#cat("Mean Specificity:", mean_specificity, "\n")
#cat("Mean J-index:", mean_j_index, "\n")

```

```{r}
# Calculate mean and median of the optimal thresholds
mean_optimal_threshold <- mean(sapply(fold_results, function(x) x$optimal_threshold))
median_optimal_threshold <- median(sapply(fold_results, function(x) x$optimal_threshold))

# Print the mean and median thresholds
#cat("Mean Optimal Threshold:", mean_optimal_threshold, "\n")
#cat("Median Optimal Threshold:", median_optimal_threshold, "\n")

```

```{r}

predictions <- predict(fitted_model, new_data = test, type = "prob")

final_predictions <- ifelse(predictions$.pred_1 > mean_optimal_threshold, 1, 0)

library(yardstick)


# Convert to a tibble if not already
test_results <- tibble(
  truth = as.factor(test$Blue_Tarp),
  estimate = as.factor(final_predictions)
)

str(test_results)
test_results$truth <- as.factor(test_results$truth)
test_results$estimate <- as.factor(test_results$estimate)
precision_res <- precision_res <- yardstick::precision(test_results, truth, estimate)


# Calculate metrics
accuracy_res <- accuracy(test_results, truth, estimate)

recall_res <- precision_res <- yardstick::recall(test_results, truth, estimate)
f1_res <- f_meas(test_results, truth, estimate)
specificity_res <- yardstick::specificity(test_results, truth, estimate)


```

```{r, results='hide', message=FALSE, warning=FALSE}
perform_cv_with_threshold <- function(model_spec, folds, dis_recipe) {
  fold_results <- list()
  
  for (i in 1:length(folds$splits)) {
    fold_data <- assessment(folds$splits[[i]])
    fold_train_data <- analysis(folds$splits[[i]])
    
    fitted_model <- workflow() %>%
      add_recipe(dis_recipe) %>%
      add_model(model_spec) %>%
      fit(data = fold_train_data)
    
    predictions <- predict(fitted_model, new_data = fold_data, type = "prob") %>%
      bind_cols(fold_data)
    
    optimal_threshold_info <- find_optimal_threshold(predictions$.pred_1, fold_data$Blue_Tarp)
    
    metrics <- calculate_metrics_at_threshold(predictions$.pred_1, fold_data$Blue_Tarp, optimal_threshold_info$optimal_threshold, "test")
    
    fold_results[[i]] <- list(metrics = metrics, optimal_threshold = optimal_threshold_info$optimal_threshold)
  }
  
  mean_optimal_threshold <- mean(sapply(fold_results, function(x) x$optimal_threshold))
  
  return(list(fold_results = fold_results, mean_optimal_threshold = mean_optimal_threshold))
}

# Perform CV with threshold optimization for each model
results_logreg <- perform_cv_with_threshold(logreg_spec, folds, dis_recipe)
results_lda <- perform_cv_with_threshold(lda_spec, folds, dis_recipe)
results_qda <- perform_cv_with_threshold(qda_spec, folds, dis_recipe)
```
Below are the performance metrics for each of the three models: Logistic Regression, QDA, and LDA on the test set with the mean of the optimal thresholds of each fold as the chosen value for threshold.
We can see that the all three models perform well on the test set with the chosen threshold values. In addition, there is an increase in performance from the models without threshold optimization. We will say that while 
```{r}
evaluate_model_on_test_set <- function(model_spec, dis_recipe, test, mean_optimal_threshold) {
  final_model <- workflow() %>%
    add_recipe(dis_recipe) %>%
    add_model(model_spec) %>%
    fit(data = train)
  
  predictions <- predict(final_model, new_data = test, type = "prob")
  
  final_predictions <- ifelse(predictions$.pred_1 > mean_optimal_threshold, 1, 0)
  
  test_results <- tibble(
    truth = as.factor(test$Blue_Tarp),
    estimate = as.factor(final_predictions)
  )
  
  metrics <- yardstick::metric_set(
    yardstick::accuracy, 
    yardstick::precision, 
    yardstick::recall, 
    yardstick::f_meas, 
    yardstick::specificity
  )(data = test_results, truth = truth, estimate = estimate)
  
  return(metrics)
}

# Evaluate each model on the test set
metrics_logreg <- evaluate_model_on_test_set(logreg_spec, dis_recipe, test, results_logreg$mean_optimal_threshold)
metrics_lda <- evaluate_model_on_test_set(lda_spec, dis_recipe, test, results_lda$mean_optimal_threshold)
metrics_qda <- evaluate_model_on_test_set(qda_spec, dis_recipe, test, results_qda$mean_optimal_threshold)

print("Metrics Log Reg")
print(metrics_logreg)
print("Metrics LDA")
print( metrics_lda)
print("Metrics QDA")
print(metrics_qda)

```
QDA performs the best among these three, since all of the performance metrics are the highest in the above table.


