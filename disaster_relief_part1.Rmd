---
title: "disaster relief part 1"
author: "Eric Rodriguez"
date: "2024-03-16"
output: html_document
---

```{r}
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```

```{r hide-code, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

```{r warning=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(discrim)
library(patchwork)
#install.packages("glmnet")
library(glmnet)
#install.packages("doParallel")
library(dplyr)
library(doParallel)
library(caret)
library(rsample)
library(MASS) 
library(discrim)
library(yardstick)
library(GGally)
```

```{r}
train <- read.csv(file="C:/Users/ericr/Downloads/HaitiPixels.csv")
```

```{r}
# Assuming all your .txt files are in the 'amaze' folder on your desktop
folder_path <- "C:/Users/ericr/OneDrive/Desktop/amaze"  # Adjust the path if necessary

# Define the column names
column_names <- c("ID", "X", "Y", "Map_X", "Map_Y", "Lat", "Lon", "B1", "B2", "B3")

# Function to read each file
read_envi_data <- function(file_name) {
  # Read in the file, skipping the first 8 lines
  data <- read.table(file_name, skip = 8, header = FALSE, fill = TRUE, 
                     col.names = column_names, check.names = FALSE, 
                     sep = "", stringsAsFactors = FALSE)
  return(data)
}

file_list <- list.files(path = folder_path, pattern = "\\.txt$", full.names = TRUE)

# Apply the custom read function to all files
df_list <- lapply(file_list, read_envi_data)

# Combine all data frames into one
test <- do.call(rbind, df_list)

test <- subset(test, select = -c(ID, X, Y, Map_X, Map_Y, Lat, Lon))

# View the top of the combined dataframe
tail(test)


```

In the RGB color model, colors with higher values tend to be dominant in the mix to produce the resultant color. For a typical representation where higher numbers represent greater intensity:


B1 likely represents Green
B2 likely represents Red
B3 likely represents Blue

```{r}
test <- test %>%
  rename(
    Green = B1,
    Red = B2,
    Blue = B3
  )


# Assign values to the Blue_Tarp column based on the specified counts for each group
test$Blue_Tarp <- c(rep(0, 979278), rep(1, 4446), rep(0, 305211), rep(0, 6828), rep(0, 295510), rep(1, 3206), rep(0, 409698))

```

```{r}
train$Class <- as.factor(train$Class)
```

```{r}
train$Blue_Tarp <- ifelse(train$Class == "Blue Tarp", 1, 0)
```

```{r}
train$Blue_Tarp <- as.factor(train$Blue_Tarp)
test$Blue_Tarp <- as.factor(test$Blue_Tarp)
```

```{r}
train <- subset(train, select = -Class)

```



```{r}
 
#Boxplots of response variable vs predictor variable

train %>%

pivot_longer(!Blue_Tarp, names_to='variable', values_to='value') %>%

ggplot(aes(x=Blue_Tarp, y=value)) +

geom_boxplot() +

facet_wrap(~variable, scales='free')

```

```{r}

#Predictor variables correlation

train%>%dplyr::select(-c(Blue_Tarp))%>%

ggpairs(aes(alpha=0.1),progress = FALSE)

```

```{r}
formula <- Blue_Tarp ~`Red`+`Green`+`Blue`
```


```{r}
# View the value counts of the Blue_Tarp column in train
table(train$Blue_Tarp)

```



```{r}
resamples <- vfold_cv(train, v=10, strata=Blue_Tarp)
metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)
```

```{r}
logreg_spec <- logistic_reg(mode="classification") %>%
set_engine('glm')
lda_spec <- discrim_linear(mode="classification") %>%
set_engine('MASS')
qda_spec <- discrim_quad(mode="classification") %>%
set_engine('MASS')
```

```{r}
dis_recipe <- recipe(formula, data=train) %>%
step_normalize(all_numeric_predictors())
```

```{r}
logreg_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(logreg_spec)
lda_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(lda_spec)
qda_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(qda_spec)


```

```{r}
logreg_cv <- fit_resamples(logreg_wf, resamples, metrics=metrics, control=cv_control)
lda_cv <- fit_resamples(lda_wf, resamples, metrics=metrics, control=cv_control)
qda_cv <- fit_resamples(qda_wf, resamples, metrics=metrics, control=cv_control)
```


```{r}
cv_metrics <- bind_rows(
collect_metrics(logreg_cv) %>% mutate(model="Logistic regression"),
collect_metrics(lda_cv) %>% mutate(model="LDA"),
collect_metrics(qda_cv) %>% mutate(model="QDA")
)

```

```{r}
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean-std_err, xmax=mean+std_err)) +
geom_point() +
geom_linerange() +
facet_wrap(~ .metric)

```


Above are the metrics for each model (Logistic Regression, LDA, and QDA) before threshold optimization. We will now perform 10 fold cross validation while optimizing the J index for each fold. The reason for choosing the J-index as a metric for threshold optimization is that it is often more reliable than something like accuracy when there are extremely unbalanced classes for the response variable. We created a function called find_optimal_threshold that iterates over each of the folds and calculates the J-index of each fold and finds the threshold that maximizes the J-index.
```{r}
#writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
#install.packages("probably")
#pak::pak("probably")

library(probably)
#pkgbuild::check_build_tools(debug = TRUE)
#install.packages("pak")
library(pkgbuild)
#install.packages("pkgbuild")
#pak::pak("tidymodels/probably")
#pkgbuild::check_build_tools(debug = TRUE)
threshold_graph <- function(model_cv, model_name) {
performance <- probably::threshold_perf(collect_predictions(model_cv), Blue_Tarp, .pred_1,
thresholds=seq(0.05, .95, 0.01), event_level="second",
metrics=metric_set(j_index, accuracy, kap))
max_metrics <- performance %>%
group_by(.metric) %>%
filter(.estimate == max(.estimate))
max_j_index_threshold <- max_metrics %>%
    filter(.metric == "j_index") %>%
    pull(.threshold)
plot <- ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
geom_line() +
geom_point(data=max_metrics, color="black") +
labs(x="Threshold", y="Metric value") +
coord_cartesian(ylim=c(0, 1.0))

 return(list(plot = plot, max_j_index_threshold = max_j_index_threshold))
}
logreg_results <- threshold_graph(logreg_cv, "Logistic regression")
plot_logreg <- logreg_results$plot
max_j_index_threshold_logreg <- logreg_results$max_j_index_threshold

# Similarly, get the results for LDA and QDA
lda_results <- threshold_graph(lda_cv, "LDA")
plot_lda <- lda_results$plot
max_j_index_threshold_lda <- lda_results$max_j_index_threshold

qda_results <- threshold_graph(qda_cv, "QDA")
plot_qda <- qda_results$plot
max_j_index_threshold_qda <- qda_results$max_j_index_threshold


```

```{r, fig.width=4, fig.height=7}
combined_plot <- plot_logreg/plot_lda/plot_qda
combined_plot
```

Above are the threshold optimization metrics. The three graphs are showing the results of our function that calculates performance metrics, such as accuracy, j_index, and kap for different metrics within the specified range of 0.05 and 0.95. The dots represent the highest threshold value for each metric.

```{r}
calculate_conf_mat <- function(model_cv,threshold) {
collect_predictions(model_cv) %>%
mutate(
.pred_class = if_else(.pred_1>threshold, 1, 0),
.pred_class = factor(.pred_class, levels=c(0, 1))
) %>%
conf_mat(truth=Blue_Tarp, estimate=.pred_class)

}

```



We perform 10 fold cross validation and threshold optimization with J-index as the metric for LDA and QDA. We are creating 10 fold cross validation splits from the training set. The data is divided into 10 parts, with each part used once as a validation set while the rest serve as a training set. We are ensuring the proportion of the two classes that is approximately equal within each fold, which is particularly useful when handling imbalanced datasets. Below are the results for each fold and for each model. 

```{r, results='hide', message=FALSE, warning=FALSE}
library(purrr)


# Define the resampling procedure
set.seed(123)




```
First we define the resampling procedure. Then, we perform cross validation. We fit the model on the fold's training data, predict on the fold's test. We resample predictions for all cv folds and train the model on our train data. Below are the confusion matrices for the resampled predictions for each model.
```{r}
cmlogreg<-calculate_conf_mat(logreg_cv,max_j_index_threshold_logreg)
print(cmlogreg)

```
TPR_LR:
```{r}
TPR <- cmlogreg$table[2,2] / (cmlogreg$table[2,2] + cmlogreg$table[1,2]) # TP / (TP + FN)
print(TPR)

```
FPR_LR:
```{r}
FPR <- cmlogreg$table[2,1] / (cmlogreg$table[2,1] + cmlogreg$table[1,1])
print(FPR)
```

```{r}
cmlda<-calculate_conf_mat(lda_cv,max_j_index_threshold_lda)
print(cmlda)
```
TPR_LDA:
```{r}
TPR1 <- cmlda$table[2,2] / (cmlda$table[2,2] + cmlda$table[1,2]) # TP / (TP + FN)
print(TPR1)
```

FPR_LDA:

```{r}
FPR1 <- cmlda$table[2,1] / (cmlda$table[2,1] + cmlda$table[1,1])
print(FPR1)
```

```{r}
cmqda<-calculate_conf_mat(qda_cv,max_j_index_threshold_qda)
print(cmqda)
```
TPR_QDA:

```{r}
TPR2 <- cmqda$table[2,2] / (cmqda$table[2,2] + cmqda$table[1,2]) # TP / (TP + FN)
print(TPR2)
```

FPR_QDA:
```{r}
FPR2 <- cmlda$table[2,1] / (cmlda$table[2,1] + cmlda$table[1,1])
print(FPR2)
```
From our confusion matrices, we see that the performances are solid, verifying what we saw from our mean performance metrics across the 10 folds for each model. 

```{r}
roc_cv_plot <- function(model_cv, model_name) {
cv_predictions <- collect_predictions(model_cv)
cv_ROC <- cv_predictions %>%
roc_curve(truth=Blue_Tarp, .pred_1, event_level="second")
autoplot(cv_ROC) +
labs(title=model_name)
}
g1 <- roc_cv_plot(logreg_cv, "Logistic regression")
g2 <- roc_cv_plot(lda_cv, "LDA")
g3 <- roc_cv_plot(qda_cv, "QDA")
g1 + g2 + g3
```

```{r}
roc_cv_data <- function(model_cv) {
cv_predictions <- collect_predictions(model_cv)
cv_predictions %>%
roc_curve(truth=Blue_Tarp, .pred_1, event_level="second")
}
bind_rows(
roc_cv_data(logreg_cv) %>% mutate(model="Logistic regression"),
roc_cv_data(lda_cv) %>% mutate(model="LDA"),
roc_cv_data(qda_cv) %>% mutate(model="QDA"),

) %>%
ggplot(aes(x=1-specificity, y=sensitivity, color=model)) +
geom_line()
```
```{r}
augmented_data <- augment(cv_fit, new_data = train)
```



There is no real need to try and optimize the above models (Logstic, QDA, and LDA) further, with step_pca as predictors, or in other words, try to reduce the dimensionality of our dataset further. The reason is because with a dataset with only 3 predictors, there will be diminishing returns from trying to use step_pca as predictors. We already have excellent performance metrics without step_pca.

```{r}
# Define the model specification
penalized_logit <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")


# Define the workflow
penalized_wf <- workflow() %>%
  add_recipe(dis_recipe) %>%
  add_model(penalized_logit)

# Define the resampling method


logreg_params <- extract_parameter_set_dials(penalized_wf) %>%
update(
# penalty=penalty(c(-3, 0.75)),
penalty=penalty(c(-3, -0.5)),
mixture=mixture(c(0, 1))
)

control=control_resamples(save_pred = TRUE)

# Tune the model
penalized_tuned <- tune_grid(
  penalized_wf,
  resamples = resamples,
  grid = grid_random(logreg_params, size=50),
  control = control
)

# Extract the best model
best_penalized <- select_best(penalized_tuned, "roc_auc")

show_best(penalized_tuned, metric='roc_auc', n=1)

optimal_parameters <- best_penalized$.config
# Train the final model with optimal parameters
final_penalized_wf <- finalize_workflow(penalized_wf, best_penalized ) %>%
  fit(train)

logreg_tuned_cv <- fit_resamples(final_penalized_wf, resamples, metrics=metrics, control=cv_control)

# Predict probabilities on the train set
#train_predictions <- augment(final_penalized_model, new_data = train)

penalized_results <- threshold_graph(logreg_tuned_cv, "Penalized LR")
plot_penalized<- penalized_results$plot
max_j_index_threshold_penalized <- penalized_results$max_j_index_threshold
plot_penalized


```
KNN model

Next we will do the KNN model. We will explore a range of K values between 1 and 30 and try to assess the best value for k based on metrics and performance with each k value within our 10 folds. We are also going to normalize the data prior to fitting the model.

```{r}
# Define the recipe
rec <- recipe(Blue_Tarp ~ ., data = train) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric())

# Define the model specification
knn_spec <- nearest_neighbor(weight_func = "rectangular", neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")


# Define the workflow
knn_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(knn_spec)

nn_params <- extract_parameter_set_dials(knn_wf) %>%
update(
neighbors=neighbors(c(2, 20))
)



# Tune the model
knn_tuned <- tune_grid(
  knn_wf,
  resamples = resamples,
  grid = grid_regular(nn_params, levels=20),
  control = control
)

# Extract the best model
best_knn <- select_best(knn_tuned, "roc_auc")

tuned_nn_wf <-finalize_workflow(knn_wf,best_knn) %>%
fit(train)

nn_cv <- fit_resamples(tuned_nn_wf, resamples, metrics=metrics, control=control_resamples(save_pred = TRUE))
# Obtain predictions using augment
#augmented_data <- augment(cv_fit, new_data = train)
#augmented_data <- tuned_nn_model %>%
  #augment(new_data = train)

knn_results <- threshold_graph(nn_cv, "KNN")
plot_knn <- knn_results$plot
max_j_index_threshold_knn <- knn_results$max_j_index_threshold
max_j_index_threshold_knn
plot_knn

```



This model is outperforming the previous three models, especially in accuracy and specificity. The performance among k values is relatively the same. It doesn't truly matter which k value we choose, because the knn model is performing well for all k values chosen, but if we had to, I would choose the k that has the highest J index. The specificity and accuracy are extremely high for this "best" value of k.

based on J-index. J index is a better metric for imbalanced classes when comparing values for k in knn.
This model has excellent performance metrics seemingly (accuracy, sensitivity, and specificity). We are training the model with the best_k.

 Below are the selection results of the 10 fold CV based on maximizing J index.


```{r}
best_params_logreg <- select_best(penalized_tuned, metric="roc_auc")
best_params_nn <- select_best(knn_tuned, metric="roc_auc")

```
Below are the metrics for the train data set at the optimal threshold of .9 for our KNN model.


The above performance metrics seem good, however, we need to evaluate the confusion matrix at this optimal threshold. 

```{r}
set.seed(123)
cmpen<-calculate_conf_mat(logreg_tuned_cv,max_j_index_threshold_penalized)
print(cmpen)
```


```{r}
TPR3 <- cmpen$table[2,2] / (cmpen$table[2,2] + cmpen$table[1,2]) # TP / (TP + FN)
print(TPR3)
```

FPR_QDA:
```{r}
FPR3 <- cmpen$table[2,1] / (cmpen$table[2,1] + cmpen$table[1,1])
print(FPR3)
```
We see that with threshold optimization, our confusion matrix produces excellent results. This means it identified many of the positive cases correctly. It also was able to identify most negative cases as negative. since we have been able to distinguish between the two classes well with knn based on the confusion matrix, we will say it is effective for our purposes.
```{r}
cmknn<-calculate_conf_mat(nn_cv,max_j_index_threshold_knn)
print(cmknn)

TPR4 <- cmknn$table[2,2] / (cmknn$table[2,2] + cmknn$table[1,2]) # TP / (TP + FN)
print(TPR4)
```


```{r}
FPR4 <- cmknn$table[2,1] / (cmknn$table[2,1] + cmknn$table[1,1])
print(FPR4)
```

We have a small FPR but our TPR is not as high as logistic regression. We will therefore say that our KNN is the best ultimately because of the high TPR and low FPR. A close contender was the logistic regression, which performed above 95% in key metrics such as accuracy, sensitivity, and specificity when looking at the mean performance across all 10 folds. It performed better than LDA and QDA when looking at these metrics alone. It also performed better in terms of TPR and FPR than LDA and QDA. The TPR is arguably the most important metric for our purposes, because we need to correctly identify the positive cases for our disaster relief efforts. Furthermore, the Logistic Regression is less computationally exhaustive and faster to run than the knn and penalized logistic regression. This may be a huge factor in future disaster relief efforts to consider.
```{r}
# Get ROC curve data for logistic regression, LDA, and QDA
roc_logreg <- roc_cv_data(logreg_cv) %>% mutate(model = "Logistic regression")
roc_lda <- roc_cv_data(lda_cv) %>% mutate(model = "LDA")
roc_qda <- roc_cv_data(qda_cv) %>% mutate(model = "QDA")

```

```{r}
library(bonsai)
library(ranger)
library(dials)
library(parsnip)
control_b=control_bayes(save_pred = TRUE)

random_wf <- workflow() %>%
add_recipe(recipe(formula, data=train)) %>%
add_model(rand_forest(mode="classification", mtry=tune(), min_n=tune()) %>%
set_engine("ranger", importance="impurity"))

parameters <- extract_parameter_set_dials(random_wf)
parameters

parameters <- extract_parameter_set_dials(random_wf) %>%
update(mtry = mtry(c(2, 8)))

tune_random <- tune_bayes(random_wf,
resamples=resamples,
metrics=metrics,
param_info=parameters, iter=25
)

#predictions <- collect_predictions(tune_random)

autoplot(tune_random)
```

```{r}
# Extract the best model
best_random<-select_best(tune_random, metric="roc_auc")

best_random_wf <- random_wf %>%
finalize_workflow(best_random)%>%
  fit(train)


random_cv <- fit_resamples(best_random_wf, resamples, metrics=metrics, control=cv_control)


#optimal_parameters1 <- best_random_wf$.config

# Predict probabilities on the train set
#train_predictions1 <- augment(best_random_wf, new_data = train)

random_results <- threshold_graph(random_cv, "RANDOM")
plot_random <- random_results$plot
max_j_index_threshold_random <- random_results$max_j_index_threshold
max_j_index_threshold_random
plot_random

```

```{r}
cmrandom<-calculate_conf_mat(random_cv,max_j_index_threshold_random)
print(cmrandom)
```
RANDOM_TPR:
```{r}
TPR5 <- cmrandom$table[2,2] / (cmrandom$table[2,2] + cmrandom$table[1,2]) # TP / (TP + FN)
print(TPR5)
```

FPR_RANDOM:
```{r}
FPR5 <- cmrandom$table[2,1] / (cmrandom$table[2,1] + cmrandom$table[1,1])
print(FPR5)
```

```{r}
svm_linear_spec <- svm_linear(mode = "classification", cost = tune(), margin=tune()) %>%
  set_engine("kernlab")

svm_poly_spec <- svm_poly(mode = "classification", cost = tune(), degree = tune()) %>%
  set_engine("kernlab")

svm_rbf_spec <- svm_rbf(cost = tune(), margin = tune(), rbf_sigma = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

workflow_linear1 <- workflow() %>%
  add_model(svm_linear_spec)  %>%
  add_recipe(dis_recipe)

workflow_poly1 <- workflow() %>%
  add_model(svm_poly_spec) %>%
  add_recipe(dis_recipe)

workflow_rbf1 <- workflow() %>%
  add_model(svm_rbf_spec) %>%
  add_recipe(dis_recipe)

```


```{r}
control1 <- control_bayes(verbose = TRUE, save_pred = TRUE)
bayes_results_linear1 <- tune_bayes(
  workflow_linear1,
  resamples = resamples,
  metrics = metrics,
  control = control1
)
param_grid <- extract_parameter_set_dials(workflow_poly1) %>%
  update(
    degree = degree_int(range=c(2, 4))
  )

grid_results_poly1 <- tune_grid(
  workflow_poly1,
  resamples = resamples,
  grid = grid_random(param_grid),
  metrics = metrics,
  control = control_grid(save_pred = TRUE)
)

parameters <- extract_parameter_set_dials(workflow_rbf1) %>%
update(rbf_sigma = rbf_sigma(c(-1.5, 1)),
cost=cost(c(-2, 5)))


set.seed(234)  # For reproducibility
rbf1_results1 <- tune_grid(
  workflow_rbf1,
  resamples = resamples,
  grid=grid_random(parameters),
  metrics = metrics,
  control = control_grid(save_pred = TRUE)
) 

show_notes(.Last.tune.result)
best_linear1 <- select_best(bayes_results_linear1, "roc_auc")
best_poly1 <- select_best(grid_results_poly1, "roc_auc")
best_rbf1 <- select_best(rbf1_results1, "roc_auc")

print(best_linear1)
print(best_poly1)
print(best_rbf1)
```

```{r}
best_linear1_model <- finalize_workflow(workflow_linear1, best_linear1)%>%
  fit(train)

best_poly1_model <- finalize_workflow(workflow_poly1, best_poly1)%>%
  fit(train)

best_rbf1_model <- finalize_workflow(workflow_rbf1, best_rbf1)%>%
  fit(train)

```

```{r}
linear1_cv <- fit_resamples(best_linear1_model, resamples, metrics=metrics, control=control)

linear1_results <- threshold_graph(linear1_cv, "Linear")
plot_linear1 <- linear1_results$plot
max_j_index_threshold_linear1 <- linear1_results$max_j_index_threshold
max_j_index_threshold_linear1
plot_linear1


```


```{r}
cmlinear1<-calculate_conf_mat(linear1_cv,max_j_index_threshold_linear1)
print(cmlinear1)
```


```{r}
TPR6 <- cmlinear1$table[2,2] / (cmlinear1$table[2,2] + cmlinear1$table[1,2]) # TP / (TP + FN)
print(TPR6)
```

FPR_RANDOM:
```{r}
FPR6 <- cmlinear1$table[2,1] / (cmlinear1$table[2,1] + cmlinear1$table[1,1])
print(FPR6)
```

```{r}
poly1_cv <- fit_resamples(best_poly1_model, resamples, metrics=metrics, control=control)

poly1_results <- threshold_graph(poly1_cv, "Poly")
plot_poly1 <- poly1_results$plot
max_j_index_threshold_poly1 <- poly1_results$max_j_index_threshold
max_j_index_threshold_poly1
plot_poly1

```
```{r}
cmpoly1<-calculate_conf_mat(poly1_cv,max_j_index_threshold_poly1)
print(cmpoly1)
```


```{r}
TPR7 <- cmpoly1$table[2,2] / (cmpoly1$table[2,2] + cmpoly1$table[1,2]) # TP / (TP + FN)
print(TPR7)
```


```{r}
FPR7 <- cmpoly1$table[2,1] / (cmpoly1$table[2,1] + cmpoly1$table[1,1])
print(FPR7)
```

```{r}
rbf1_cv <- fit_resamples(best_rbf1_model, resamples, metrics=metrics, control=control)

rbf1_results <- threshold_graph(rbf1_cv, "RBF")
plot_rbf1 <- rbf1_results$plot
max_j_index_threshold_rbf1 <- rbf1_results$max_j_index_threshold
max_j_index_threshold_rbf1
plot_rbf1


```


```{r}
cmrbf1<-calculate_conf_mat(rbf1_cv,max_j_index_threshold_rbf1)
print(cmrbf1)
```


```{r}
TPR8 <- cmrbf1$table[2,2] / (cmrbf1$table[2,2] + cmrbf1$table[1,2]) # TP / (TP + FN)
print(TPR8)
```

FPR_RANDOM:
```{r}
FPR8 <- cmrbf1$table[2,1] / (cmrbf1$table[2,1] + cmrbf1$table[1,1])
print(FPR8)
```

The best model is the random forest model based on the fact that it has the highest TPR. 99.8 percent of all blue tarps were correctly identified.



```{r}
g4 <- roc_cv_plot(logreg_tuned_cv, "Logistic regression Tuned")
g5 <- roc_cv_plot(nn_cv, "KNN")
g6<-roc_cv_plot(random_cv, "Random Forest")
g7<-roc_cv_plot(linear1_cv, "Linear Kernel SVM")
g8<-roc_cv_plot(poly1_cv, "Polynomial Kernel SVM")
g9<-roc_cv_plot(rbf1_cv, "RBF Kernel")


```

```{r}
roc_cv_data <- function(model_cv) {
cv_predictions <- collect_predictions(model_cv)
cv_predictions %>%
roc_curve(truth=Blue_Tarp, .pred_1, event_level="second")
}
bind_rows(
roc_cv_data(logreg_cv) %>% mutate(model="Logistic regression"),
roc_cv_data(lda_cv) %>% mutate(model="LDA"),
roc_cv_data(qda_cv) %>% mutate(model="QDA"),
roc_cv_data(logreg_tuned_cv)%>% mutate(model="Logistic regression Tuned"),
roc_cv_data(nn_cv)%>%mutate(model="KNN"),
roc_cv_data(random_cv)%>%mutate(model="Random"),
roc_cv_data(linear1_cv)%>%mutate(model="Linear Kernel SVM"),
roc_cv_data(poly1_cv)%>%mutate(model= "Polynomial Kernel SVM"),
roc_cv_data(rbf1_cv)%>%mutate(model= "RBF Kernel")
) %>%
ggplot(aes(x=1-specificity, y=sensitivity, color=model)) +
geom_line()
```
```{r}
library(broom)


# Fit models
logreg_fit <- logreg_wf %>%
  fit(data = train)

lda_fit <- lda_wf %>%
  fit(data = train)

qda_fit <- qda_wf %>%
  fit(data = train)

augmented_test_data_logreg <- augment(logreg_fit, new_data = test)
```

```{r}
cv_metrics1 <- bind_rows(
collect_metrics(logreg_cv) %>% mutate(model="Logistic regression"),
collect_metrics(lda_cv) %>% mutate(model="LDA"),
collect_metrics(qda_cv) %>% mutate(model="QDA"),
collect_metrics(linear1_cv) %>% mutate(model="Linear Kernel"),
collect_metrics(poly1_cv) %>% mutate(model="Poly Kernel"),
collect_metrics(rbf1_cv) %>% mutate(model="RBF Kernel"),
show_best(penalized_tuned, metric="accuracy", 1) %>%
mutate(model="Logistic regression tuned"),
show_best(penalized_tuned, metric="roc_auc", 1) %>%
mutate(model="Logistic regression tuned"),
show_best(knn_tuned, metric="accuracy", 1) %>%
mutate(model="Nearest neighbors tuned"),
show_best(knn_tuned, metric="roc_auc", 1) %>%
mutate(model="Nearest neighbors tuned"),

)

ggplot(cv_metrics1, aes(x=mean, y=model, xmin=mean-std_err, xmax=mean+std_err)) +
geom_point() +
geom_linerange() +
facet_wrap(~ .metric)

```
```{r}
cv_metrics1
```

```{r}
calculate_metrics <- function(model, test, model_name) {
  print(paste("Calculating metrics for", model_name, "on test set..."))
  test_metrics <- bind_cols(
    model = model_name,
    dataset = "test",
    metrics = yardstick::metrics(model %>% augment(test), truth = Blue_Tarp, estimate = .pred_class),
    roc_auc = yardstick::roc_auc(model %>% augment(test), truth = Blue_Tarp, .pred_1, event_level = "second")
  )
  
  return(test_metrics)
}
disaster_metrics <- bind_rows(
  calculate_metrics(logreg_fit, test, "Log Reg Untuned"),
  calculate_metrics(lda_fit, test, "LDA Untuned"),
  calculate_metrics(qda_fit, test, "QDA Untuned"),
  calculate_metrics(final_penalized_wf, test, "Penalized Logistic Regression"),
  calculate_metrics(tuned_nn_wf, test, "KNN Tuned"),
  calculate_metrics(best_random_wf, test, "Random Forest"),
  calculate_metrics(best_linear1_model, test, "Linear Kernel"),
  calculate_metrics(best_poly1_model, test, "Poly Kernel"),
  calculate_metrics(best_rbf1_model, test, "RBF Kernel")
)


  
drop <- c(".estimator...4",".estimator...7",".metric...3",".metric...6")


filtered_metrics = filtered_metrics[,!(names(filtered_metrics) %in% drop)]


filtered_metrics <- filtered_metrics %>%
  rename(
    Accuracy = .estimate...5,
    ROC_AUC = .estimate...8
  )
print(filtered_metrics)
print(names(filtered_metrics))

```

```{r}

# Calculate ROC curve for each model
roc_curves <- bind_rows(
  roc_curve(augment(logreg_fit, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "Logreg Untuned"),
  roc_curve(augment(lda_fit, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "LDA Untuned"),
  roc_curve(augment(qda_fit, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "QDA Untuned"),
  roc_curve(augment(final_penalized_wf, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "Penalized Logistic Regression"),
  roc_curve(augment(tuned_nn_wf, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "Tuned KNN"),
  roc_curve(augment(best_random_wf, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "Random Forest"),
  roc_curve(augment(best_linear1_model, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "Linear Kernel"),
  roc_curve(augment(best_poly1_model, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "Poly Kernel"),
  roc_curve(augment(best_rbf1_model, test), Blue_Tarp, .pred_1, event_level = "second") %>%
    mutate(model = "RBF Kernel")
)

# Plot ROC curves
ggplot(roc_curves, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "False Positive Rate", y = "True Positive Rate", color = "Model") +
  theme_minimal() +
  ggtitle("ROC Curves for Different Models on Test Set")

```


```{r}
stopCluster(cl)
registerDoSEQ()

```



