---
title: "part1 Disaster Relief"
author: "Eric Rodriguez"
date: "2024-03-13"
output: html_document
---

```{r hide-code, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

```{r}
library(tidymodels)
library(tidyverse)
library(discrim)
library(patchwork)
#install.packages("glmnet")
library(glmnet)
#install.packages("doParallel")
library(dplyr)
library(doParallel)
library(caret)
library(rsample)
library(MASS) 
library(discrim)
library(yardstick)
```

```{r}
df <- read.csv(file="C:/Users/ericr/Downloads/HaitiPixels.csv")
```

```{r}
df$Class <- as.factor(df$Class)
```

```{r}
df$Blue_Tarp <- ifelse(df$Class == "Blue Tarp", 1, 0)
```

```{r}
df$Blue_Tarp <- as.factor(df$Blue_Tarp)
```

```{r}
df <- subset(df, select = -Class)

```

```{r}
set.seed(123)  # Set seed for reproducibility
disrelief_split <- initial_split(df, prop=0.8, strata=Blue_Tarp)
train <- training(disrelief_split)
test <- testing(disrelief_split)
```

```{r}
formula <- Blue_Tarp ~`Red`+`Green`+`Blue`
```

```{r}
logreg_model <- logistic_reg(mode="classification") %>% 
        set_engine("glm") %>% 
        fit(formula, train)

lda_model <- discrim_linear(mode="classification") %>% 
        set_engine("MASS") %>%
        fit(formula, train)
qda_model <- discrim_quad(mode="classification") %>% 
        set_engine("MASS") %>%
        fit(formula, train)
```


```{r}
# Check the output structure of the logistic regression model
#str(logreg_model)

```

```{r}
train$Blue_Tarp <- as.factor(train$Blue_Tarp)
test$Blue_Tarp <- as.factor(test$Blue_Tarp)
```

```{r}
# View the value counts of the Blue_Tarp column in train
table(train$Blue_Tarp)

```



```{r}
resamples <- vfold_cv(train, v=10, strata=Blue_Tarp)
metrics <- metric_set(roc_auc, accuracy)
cv_control <- control_resamples(save_pred=TRUE)
```

```{r}
logreg_spec <- logistic_reg(mode="classification") %>%
set_engine('glm')
lda_spec <- discrim_linear(mode="classification") %>%
set_engine('MASS')
qda_spec <- discrim_quad(mode="classification") %>%
set_engine('MASS')
```

```{r}
dis_recipe <- recipe(formula, data=train) %>%
step_normalize(all_numeric_predictors())
```

```{r}
logreg_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(logreg_spec)
lda_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(lda_spec)
qda_wf <- workflow() %>%
add_recipe(dis_recipe) %>%
add_model(qda_spec)

```

```{r}
logreg_cv <- fit_resamples(logreg_wf, resamples, metrics=metrics, control=cv_control)
lda_cv <- fit_resamples(lda_wf, resamples, metrics=metrics, control=cv_control)
qda_cv <- fit_resamples(qda_wf, resamples, metrics=metrics, control=cv_control)
```

```{r}
cv_metrics <- bind_rows(
collect_metrics(logreg_cv) %>% mutate(model="Logistic regression"),
collect_metrics(lda_cv) %>% mutate(model="LDA"),
collect_metrics(qda_cv) %>% mutate(model="QDA")
)

```

```{r}
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean-std_err, xmax=mean+std_err)) +
geom_point() +
geom_linerange() +
facet_wrap(~ .metric)

```


Above are the metrics for each model (Logistic Regression, LDA, and QDA) before threshold optimization. We will now perform 10 fold cross validation while optimizing the J index for each fold. The reason for choosing the J-index as a metric for threshold optimization is that it is often more reliable than something like accuracy when there are extremely unbalanced classes for the response variable. We created a function called find_optimal_threshold that iterates over each of the folds and calculates the J-index of each fold and finds the threshold that maximizes the J-index.

```{r}
find_optimal_threshold <- function(predictions, truth) {
  # Create a range of thresholds
  thresholds <- seq(0, 1, by = 0.01)
  
  # Initialize vectors to store J-index values for each threshold
  j_index_values <- numeric(length(thresholds))
  
  # Iterate over each threshold
  for (i in seq_along(thresholds)) {
    # Convert probabilities to binary predictions based on the threshold
    binary_predictions <- ifelse(predictions > thresholds[i], TRUE, FALSE)
    
    # Calculate confusion matrix
    confusion_matrix <- table(binary_predictions, truth)
    
    # Ensure there are enough samples in each class for calculations
    if (ncol(confusion_matrix) != 2 || nrow(confusion_matrix) != 2) {
      j_index_values[i] <- 0  # Set J-index to 0 if confusion matrix dimensions are not as expected
    } else if (sum(confusion_matrix[2, ]) == 0 || sum(confusion_matrix[, 2]) == 0) {
      j_index_values[i] <- 0  # Set J-index to 0 if sensitivity or specificity is undefined
    } else {
      # Calculate sensitivity and specificity
      sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
      specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
      
      # Calculate J-index
      j_index_values[i] <- sensitivity + specificity - 1
    }
  }
  
  # Find the optimal threshold that maximizes the J-index
  optimal_threshold <- thresholds[which.max(j_index_values)]
  optimal_j_index <- max(j_index_values)
  
  return(list(optimal_threshold = optimal_threshold, optimal_j_index = optimal_j_index))
}
```

```{r}
calculate_metrics_at_threshold <- function(predictions, truth, threshold, data = "train") {
  binary_predictions <- ifelse(predictions > threshold, TRUE, FALSE)
  confusion_matrix <- table(binary_predictions, truth)
  
  # Ensure there are enough samples in each class for calculations
  if (ncol(confusion_matrix) != 2 || nrow(confusion_matrix) != 2) {
    return(list(accuracy = 0, sensitivity = 0, specificity = 0, j_index = 0))  # Set all metrics to 0 if confusion matrix dimensions are not as expected
  } else if (sum(confusion_matrix[2, ]) == 0 || sum(confusion_matrix[, 2]) == 0) {
    return(list(accuracy = 0, sensitivity = 0, specificity = 0, j_index = 0))  # Set all metrics to 0 if sensitivity or specificity is undefined
  } else {
    # Calculate accuracy
    accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
    
    # Calculate sensitivity
    sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
    
    # Calculate specificity
    specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
    
    # Calculate J-index
    j_index <- sensitivity + specificity - 1
    
    # Print whether it's for train or test data
    cat("Metrics for", data, "data at threshold", threshold, ":\n")
    cat("Accuracy:", accuracy, "\n")
    cat("Sensitivity:", sensitivity, "\n")
    cat("Specificity:", specificity, "\n")
    cat("J-index:", j_index, "\n\n")
    
    return(list(accuracy = accuracy, sensitivity = sensitivity, specificity = specificity, j_index = j_index))
  }
}
```

We perform 10 fold cross validation and threshold optimization with J-index as the metric for LDA and QDA. We are creating 10 fold cross validation splits from the training set. The data is divided into 10 parts, with each part used once as a validation set while the rest serve as a training set. We are ensuring the proportion of the two classes that is approximately equal within each fold, which is particularly useful when handling imbalanced datasets. Below are the results for each fold and for each model. 
```{r, results='hide', message=FALSE, warning=FALSE}
library(purrr)


# Define the resampling procedure
set.seed(123)
folds <- vfold_cv(train, v = 10, strata = Blue_Tarp)

# Initialize a list to store results
fold_results <- list()

# Perform the cross-validation
for (i in 1:length(folds$splits)) {
  # Get the train and test split for the current fold
  fold_data <- assessment(folds$splits[[i]])
  fold_train_data <- analysis(folds$splits[[i]])
  
  # Fit the model on the fold's training data
  fitted_model <- workflow() %>%
    add_recipe(dis_recipe) %>%
    add_model(logreg_spec) %>%
    fit(data = fold_train_data)
  
  # Predict on the fold's test data
  predictions <- predict(fitted_model, new_data = fold_data, type = "prob") %>%
    bind_cols(fold_data)
  
  # Find the optimal threshold for this fold
  optimal_threshold_info <- find_optimal_threshold(predictions$.pred_1, fold_data$Blue_Tarp)
  
  # Calculate metrics at the optimal threshold
  metrics <- calculate_metrics_at_threshold(predictions$.pred_1, fold_data$Blue_Tarp, optimal_threshold_info$optimal_threshold, "test")
  
  # Store the metrics and the optimal threshold for this fold
  fold_results[[i]] <- list(metrics = metrics, optimal_threshold = optimal_threshold_info$optimal_threshold)
}



```

```{r}
# Calculate mean of each metric across all folds
mean_accuracy <- mean(sapply(fold_results, function(x) x$metrics$accuracy))
mean_sensitivity <- mean(sapply(fold_results, function(x) x$metrics$sensitivity))
mean_specificity <- mean(sapply(fold_results, function(x) x$metrics$specificity))
mean_j_index <- mean(sapply(fold_results, function(x) x$metrics$j_index))

# Print the aggregated metrics
#cat("Mean Accuracy:", mean_accuracy, "\n")
#cat("Mean Sensitivity:", mean_sensitivity, "\n")
#cat("Mean Specificity:", mean_specificity, "\n")
#cat("Mean J-index:", mean_j_index, "\n")

```

```{r}
# Calculate mean and median of the optimal thresholds
mean_optimal_threshold <- mean(sapply(fold_results, function(x) x$optimal_threshold))
median_optimal_threshold <- median(sapply(fold_results, function(x) x$optimal_threshold))

# Print the mean and median thresholds
#cat("Mean Optimal Threshold:", mean_optimal_threshold, "\n")
#cat("Median Optimal Threshold:", median_optimal_threshold, "\n")

```

```{r}

predictions <- predict(fitted_model, new_data = test, type = "prob")

final_predictions <- ifelse(predictions$.pred_1 > mean_optimal_threshold, 1, 0)

library(yardstick)


# Convert to a tibble if not already
test_results <- tibble(
  truth = as.factor(test$Blue_Tarp),
  estimate = as.factor(final_predictions)
)

str(test_results)
test_results$truth <- as.factor(test_results$truth)
test_results$estimate <- as.factor(test_results$estimate)
precision_res <- precision_res <- yardstick::precision(test_results, truth, estimate)


# Calculate metrics
accuracy_res <- accuracy(test_results, truth, estimate)

recall_res <- precision_res <- yardstick::recall(test_results, truth, estimate)
f1_res <- f_meas(test_results, truth, estimate)
specificity_res <- yardstick::specificity(test_results, truth, estimate)


```

We created a function called perform_cv_with_threshold for our three models, which allows us to assess our models performance at our optimal threshold during 10 fold CV.
```{r, results='hide', message=FALSE, warning=FALSE}
perform_cv_with_threshold <- function(model_spec, folds, dis_recipe) {
  fold_results <- list()
  
  for (i in 1:length(folds$splits)) {
    fold_data <- assessment(folds$splits[[i]])
    fold_train_data <- analysis(folds$splits[[i]])
    
    fitted_model <- workflow() %>%
      add_recipe(dis_recipe) %>%
      add_model(model_spec) %>%
      fit(data = fold_train_data)
    
    predictions <- predict(fitted_model, new_data = fold_data, type = "prob") %>%
      bind_cols(fold_data)
    
    optimal_threshold_info <- find_optimal_threshold(predictions$.pred_1, fold_data$Blue_Tarp)
    
    metrics <- calculate_metrics_at_threshold(predictions$.pred_1, fold_data$Blue_Tarp, optimal_threshold_info$optimal_threshold, "test")
    
    fold_results[[i]] <- list(metrics = metrics, optimal_threshold = optimal_threshold_info$optimal_threshold)
  }
  
  mean_optimal_threshold <- mean(sapply(fold_results, function(x) x$optimal_threshold))
  
  return(list(fold_results = fold_results, mean_optimal_threshold = mean_optimal_threshold))
}

# Perform CV with threshold optimization for each model
results_logreg <- perform_cv_with_threshold(logreg_spec, folds, dis_recipe)
results_lda <- perform_cv_with_threshold(lda_spec, folds, dis_recipe)
results_qda <- perform_cv_with_threshold(qda_spec, folds, dis_recipe)
```
Below are the performance metrics for each of the three models: Logistic Regression, QDA, and LDA on the test set with the mean of the optimal thresholds of each fold as the chosen value for threshold.
We can see that the all three models perform well on the test set with the chosen threshold values. In addition, there is an increase in performance from the models without threshold optimization. 
```{r}
evaluate_model_on_test_set <- function(model_spec, dis_recipe, test, mean_optimal_threshold) {
  final_model <- workflow() %>%
    add_recipe(dis_recipe) %>%
    add_model(model_spec) %>%
    fit(data = train)
  
  predictions <- predict(final_model, new_data = test, type = "prob")
  
  final_predictions <- ifelse(predictions$.pred_1 > mean_optimal_threshold, 1, 0)
  
  test_results <- tibble(
    truth = as.factor(test$Blue_Tarp),
    estimate = as.factor(final_predictions)
  )
  
  metrics <- yardstick::metric_set(
    yardstick::accuracy, 
    yardstick::precision, 
    yardstick::recall, 
    yardstick::f_meas, 
    yardstick::specificity
  )(data = test_results, truth = truth, estimate = estimate)
  
  return(metrics)
}

# Evaluate each model on the test set
metrics_logreg <- evaluate_model_on_test_set(logreg_spec, dis_recipe, test, results_logreg$mean_optimal_threshold)
metrics_lda <- evaluate_model_on_test_set(lda_spec, dis_recipe, test, results_lda$mean_optimal_threshold)
metrics_qda <- evaluate_model_on_test_set(qda_spec, dis_recipe, test, results_qda$mean_optimal_threshold)

print("Metrics Log Reg")
print(metrics_logreg)
print("Metrics LDA")
print( metrics_lda)
print("Metrics QDA")
print(metrics_qda)

```
QDA performs the best among these three, since all of the performance metrics are the highest in the above table.

```{r}
roc_cv_plot <- function(model_cv, model_name) {
cv_predictions <- collect_predictions(model_cv)
cv_ROC <- cv_predictions %>%
roc_curve(truth=Blue_Tarp, .pred_1, event_level="second")
autoplot(cv_ROC) +
labs(title=model_name)
}
g1 <- roc_cv_plot(logreg_cv, "Logistic regression")
g2 <- roc_cv_plot(lda_cv, "LDA")
g3 <- roc_cv_plot(qda_cv, "QDA")
g1 + g2 + g3
```

```{r}
roc_cv_data <- function(model_cv) {
cv_predictions <- collect_predictions(model_cv)
cv_predictions %>%
roc_curve(truth=Blue_Tarp, .pred_1, event_level="second")
}
bind_rows(
roc_cv_data(logreg_cv) %>% mutate(model="Logistic regression"),
roc_cv_data(lda_cv) %>% mutate(model="LDA"),
roc_cv_data(qda_cv) %>% mutate(model="QDA")
) %>%
ggplot(aes(x=1-specificity, y=sensitivity, color=model)) +
geom_line()
```
There is no real need to try and optimize the above models (Logstic, QDA, and LDA) further, with step_pca as predictors, or in other words, try to reduce the dimensionality of our dataset further. The reason is because with a dataset with only 3 predictors, there will be diminishing returns from trying to use step_pca as predictors. We already have excellent performance metrics without step_pca.

KNN model

Next we will do the KNN model. We will explore a range of K values between 2 and 100 and try to assess the best value for k based on metrics and performance with each k value. We are also going to normalize the data prior to fitting the model.
```{r}
# Select the features you want to normalize
selected_features_train <- train[, c("Red","Green","Blue")]

selected_features_test<-test[, c("Red","Green","Blue")]
# Normalize the selected features using z-score normalization
scaled_features_train <- scale(selected_features_train)
scaled_features_test<-scale(selected_features_test)
# Convert Blue_Tarp to a data frame
train_Blue_Tarp_df <- data.frame(Blue_Tarp = train$Blue_Tarp)
test_Blue_Tarp_df<-data.frame(Blue_Tarp=test$Blue_Tarp)
# Bind the scaled features with the Blue_Tarp variable and convert Blue_Tarp back to a factor
normalized_train_data <- cbind(scaled_features_train, train_Blue_Tarp_df)
normalized_train_data$Blue_Tarp <- factor(normalized_train_data$Blue_Tarp)

normalized_test_data<-cbind(scaled_features_test, test_Blue_Tarp_df)
normalized_test_data$Blue_Tarp <- factor(normalized_test_data$Blue_Tarp)

# Ensure Blue_Tarp is a factor with 2 levels
levels(normalized_train_data$Blue_Tarp) <- c("False", "True")
levels(normalized_test_data$Blue_Tarp)<-c("False","True")



```

```{r}
ctrl <- trainControl(method = "cv",   # Use cross-validation
                     number = 10)      # 5-fold cross-validation

# Train several k-nearest neighbor models
k_values <- c(2, 5, 10, 20, 50, 75, 100)

resultsdf <- data.frame(k = numeric(),
                      Accuracy = numeric(),
                      Sensitivity = numeric(),
                      Specificity = numeric(),
                      Precision = numeric(),
                      F1_Score = numeric())

for (k in k_values) {
  # Train the KNN model
  knn_model <- train(Blue_Tarp ~ .,           # Formula: dependent variable ~ predictors
                     data = normalized_train_data,   # Training data
                     method = "knn",    # Use k-nearest neighbors algorithm
                     trControl = ctrl,  # Use the defined control parameters
                     tuneGrid = data.frame(k = k)) # Set the number of neighbors
    
  # Make predictions
  predictions_knn <- predict(knn_model, newdata = normalized_test_data)

  # Compute confusion matrix
  confusion_matrix <- table(predictions_knn, normalized_test_data$Blue_Tarp)

  # Compute metrics
  accuracy <- sum(predictions_knn == normalized_test_data$Blue_Tarp) / length(predictions_knn)
  sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
  specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
  precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
  f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)

  # Store results
  resultsdf <- rbind(resultsdf, data.frame(k = k, Accuracy = accuracy, Sensitivity = sensitivity,
                                           Specificity = specificity, Precision = precision,
                                           F1_Score = f1_score))
}

# Print results
print(resultsdf)

```

This model is outperforming the previous three models, especially in accuracy and specificity. The performance among k values is relatively the same. It doesn't truly matter which k value we choose, because the knn model is performing well for all k values chosen, but if we had to, I would go with a k value of 50. The specificity and accuracy are extremely high for this chosen value of k.

```{r}
library(pROC)

# Train the KNN model with k = 50
knn_model <- train(formula,           
                   data = normalized_train_data,   
                   method = "knn",    
                   trControl = ctrl,  
                   tuneGrid = data.frame(k = 50))

# Make predictions
preds <- predict(knn_model, newdata = normalized_test_data, type = "prob")

# Extract the predicted probabilities for the positive class
predicted_probabilities <- preds[, "True"]

# Compute ROC curve
roc_curve <- roc(normalized_test_data$Blue_Tarp, predicted_probabilities)

# Plot ROC curve
plot(roc_curve, main = "ROC Curve for KNN (k = 50)",
     col = "blue", lwd = 2, print.auc = TRUE)

```
We have a great looking roc curve here. This means that the performance on unseen data is extremely high. However, it is still critical to further validate our KNN model with 10 fold CV. Below are the selection results of the 10 fold CV based on maximizing J index.

```{r}
# Predict probabilities for the positive class
predictions_KNN_train <- predict(knn_model, new_data = train, type = "prob", data="train")

# Check column names
colnames(predictions_KNN_train)

predictions_KNN_train <- predict(knn_model, new_data = train, type = "prob")[, "True"]
optimal_threshold_KNN_train <- find_optimal_threshold(predictions_KNN_train, train$Blue_Tarp)

print(optimal_threshold_KNN_train)
```
Below are the metrics for the train data set at the optimal threshold of .88.

```{r}
metrics_KNN_train <- calculate_metrics_at_threshold(predictions_KNN_train, train$Blue_Tarp, optimal_threshold_KNN_train$optimal_threshold)

cat("KNN Model (Train):\n")
cat("Accuracy:", metrics_KNN_train$accuracy, "\n")
cat("Sensitivity:", metrics_KNN_train$sensitivity, "\n")
cat("Specificity:", metrics_KNN_train$specificity, "\n")
cat("J-index:", metrics_KNN_train$j_index, "\n\n")


```
Below are the results of our knn model on the test set at the threshold of .88.
```{r}
knn_model_test <- train(formula,           
                   data = normalized_test_data,   
                   method = "knn",    
                   trControl = ctrl,  
                   tuneGrid = data.frame(k = 50))
#test <- subset(test, select = -Blue_Tarp)
predictions_KNN_test <- predict(knn_model_test, new_data = test)
predictions_KNN_test <- predict(knn_model_test, new_data = test, type = "prob")[, "True"]

optimal_threshold_KNN_test <- find_optimal_threshold(predictions_KNN_test, normalized_test_data$Blue_Tarp)
metrics_KNN_test <- calculate_metrics_at_threshold(predictions_KNN_test, normalized_test_data$Blue_Tarp, optimal_threshold_KNN_test$optimal_threshold)

cat("KNN Model (test):\n")
cat("Accuracy:", metrics_KNN_test$accuracy, "\n")
cat("Sensitivity:", metrics_KNN_test$sensitivity, "\n")
cat("Specificity:", metrics_KNN_test$specificity, "\n")
cat("J-index:", metrics_KNN_test$j_index, "\n\n")

```
Here are the results of the 10 fold CV with our KNN model.
```{r}
library(caret)

# Assuming normalized_train_data is your prepared training data
# and that "Blue_Tarp" is the target variable in your dataset

# Set up cross-validation control
ctrl <- trainControl(method = "cv",
                     number = 10,      # Number of folds
                     summaryFunction = twoClassSummary,  # For ROC AUC
                     classProbs = TRUE)  # To calculate class probabilities

# Set the seed for reproducibility
set.seed(123)

# Choose a range of k values to try
k_values <- data.frame(k = c(2, 5, 10, 20, 50, 75, 100))

# Train the KNN model using cross-validation
knn_cv_model <- train(Blue_Tarp ~ .,
                      data = normalized_train_data,
                      method = "knn",
                      tuneGrid = k_values,
                      trControl = ctrl,
                      metric = "ROC")

# Look at the cross-validation results
print(knn_cv_model)

```
ROC was used to select the optimal k value during this 10 fold CV and the final value was k=50.

For our elasticnet penalized logistic regression, we first have to prepare the data for glmnet. This means turning our normalized train and test data into matrices.


```{r}
x_train <- as.matrix(normalized_train_data[, -which(names(normalized_train_data) == "Blue_Tarp")])
y_train <- normalized_train_data$Blue_Tarp

```

Next, we set up the training control, specifying 10-fold cross-validation and the ROC metric for evaluating model performance. ROC is a good metric for model evaluation, especially when dealing with imbalanced classes. 

```{r}
set.seed(123)  # for reproducibility
cv_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)

```

We train the  elastic net model using th train function from the caret package, specifying a grid of alpha and lambda values.
```{r}
# Define the range of lambda values
lambda_grid <- 10^seq(-3, 3, length = 100)

# Define alpha values (mixing percentages between L1 and L2 penalties)
alpha_grid <- seq(0, 1, by = 0.1)

# Fit the elastic net model with cross-validation
set.seed(123)  # for reproducibility
enet_model <- train(
  x = x_train,
  y = y_train,
  method = "glmnet",
  trControl = cv_control,
  tuneLength = 10,  # Select 10 lambda values; adjust if necessary
  tuneGrid = expand.grid(alpha = alpha_grid, lambda = lambda_grid),
  metric = "ROC",
  preProcess = c("center", "scale"),  # Ensuring features are normalized
  family = "binomial"
)

```

```{r}
# Get the best hyperparameters
best_hyperparameters <- enet_model$bestTune

# Get the best model's performance
best_model_performance <- max(enet_model$results$ROC)

# Print the results
print(best_hyperparameters)
print(best_model_performance)

```
Above are the model selection results, or in other words, the values of alpha and lambda that produced the highest roc value on the train set. Now we evaluate the model on the test set by using the trained model.

```{r}
# Prepare the test set
x_test <- as.matrix(normalized_test_data[, -which(names(normalized_test_data) == "Blue_Tarp")])
y_test <- normalized_test_data$Blue_Tarp

# Make predictions on the test set
test_predictions <- predict(enet_model, x_test)

# Calculate performance metrics
conf_matrix <- confusionMatrix(test_predictions, y_test, positive = "True")

print(conf_matrix)
```
This model doesn't perform as well as the KNN model in Sensitivity or accuracy. We will say that this model is the worst model out of our five. 
